---
- name: Import optional extra_args file
  include_vars: "{{ item }}"
  ignore_errors: yes
  with_items:
    - "../extra_vars.yaml"
  tags: vars

- name: Set local directories
  set_fact:
    local_results_dir: "{{ topdir_path }}/workflows/ai/results"
    local_scripts_dir: "{{ topdir_path }}/workflows/ai/scripts"
  run_once: true
  delegate_to: localhost

- name: Create local directories if they don't exist
  file:
    path: "{{ item }}"
    state: directory
    mode: '0755'
  loop:
    - "{{ local_results_dir }}"
    - "{{ local_scripts_dir }}"
  run_once: true
  delegate_to: localhost

- name: Create analysis directory
  file:
    path: "{{ ai_benchmark_results_dir }}/analysis"
    state: directory
    mode: '0755'
  become: yes

- name: Copy analysis scripts to scripts directory
  copy:
    src: "{{ item }}"
    dest: "{{ local_scripts_dir }}/{{ item }}"
    mode: '0755'
  loop:
    - analyze_results.py
    - generate_graphs.py
    - generate_html_report.py
  run_once: true
  delegate_to: localhost

- name: Generate analysis configuration
  template:
    src: analysis_config.json.j2
    dest: "{{ local_scripts_dir }}/analysis_config.json"
    mode: '0644'
  run_once: true
  delegate_to: localhost
  when: ai_benchmark_enable_graphing|bool

- name: Check if benchmark results exist
  stat:
    path: "{{ ai_benchmark_results_dir }}"
  register: results_dir_check

- name: Find benchmark result files on remote host
  find:
    paths: "{{ ai_benchmark_results_dir }}"
    patterns: "results_*.json"
  register: remote_results
  when: results_dir_check.stat.exists

- name: Collect result files from all hosts
  fetch:
    src: "{{ item.path }}"
    dest: "{{ local_results_dir }}/{{ item.path | basename }}"
    flat: yes
  loop: "{{ remote_results.files | default([]) }}"
  when:
    - results_dir_check.stat.exists
    - remote_results.files is defined

- name: Check if any results were collected
  find:
    paths: "{{ local_results_dir }}"
    patterns: "*results_*.json"
  register: collected_results
  run_once: true
  delegate_to: localhost

- name: Display message if no results found
  debug:
    msg: |
      No benchmark results found to analyze.
      Please run 'make ai-tests' first to generate benchmark results.
  when: collected_results.files is not defined or collected_results.files | length == 0
  run_once: true
  delegate_to: localhost

- name: Run results analysis
  command: >
    python3 {{ local_scripts_dir }}/analyze_results.py
    --results-dir {{ local_results_dir }}
    --output-dir {{ local_results_dir }}
    {% if ai_benchmark_enable_graphing|bool %}--config {{ local_scripts_dir }}/analysis_config.json{% endif %}
  register: analysis_result
  run_once: true
  delegate_to: localhost
  when: collected_results.files is defined and collected_results.files | length > 0


- name: Create graphs directory
  file:
    path: "{{ local_results_dir }}/graphs"
    state: directory
    mode: '0755'
  run_once: true
  delegate_to: localhost
  when:
    - collected_results.files is defined
    - collected_results.files | length > 0

- name: Generate performance graphs
  command: >
    python3 {{ local_scripts_dir }}/generate_graphs.py
    {{ local_results_dir }}
    {{ local_results_dir }}/graphs
  register: graph_generation_result
  run_once: true
  delegate_to: localhost
  when:
    - collected_results.files is defined
    - collected_results.files | length > 0
    - ai_benchmark_enable_graphing|bool

- name: Generate HTML report
  command: >
    python3 {{ local_scripts_dir }}/generate_html_report.py
    {{ local_results_dir }}
    {{ local_results_dir }}/graphs
    {{ local_results_dir }}/benchmark_report.html
  register: html_generation_result
  run_once: true
  delegate_to: localhost
  when:
    - collected_results.files is defined
    - collected_results.files | length > 0

- name: Display analysis completion message
  debug:
    msg: |
      Benchmark analysis completed!
      Results available in: {{ local_results_dir }}/
      Summary report: {{ local_results_dir }}/benchmark_summary.txt
      HTML report: {{ local_results_dir }}/benchmark_report.html
      {% if ai_benchmark_enable_graphing|bool %}
      Graphs generated in: {{ local_results_dir }}/graphs/
      {% endif %}

      To view the HTML report:
      - Open {{ local_results_dir }}/benchmark_report.html in a web browser
  run_once: true
  delegate_to: localhost
  when:
    - collected_results.files is defined
    - collected_results.files | length > 0
    - analysis_result is defined
    - analysis_result.rc == 0
