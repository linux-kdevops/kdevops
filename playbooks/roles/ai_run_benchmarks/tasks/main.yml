---
- name: Import optional extra_args file
  ansible.builtin.include_vars: "{{ item }}"
  # TODO: Review - was ignore_errors: true
  failed_when: false  # Always succeed - review this condition
  with_items:
    - "../extra_vars.yaml"
  tags: vars

- name: Clean up any stale lock files from previous runs (force mode)
  ansible.builtin.file:
    path: "{{ ai_benchmark_results_dir }}/.benchmark.lock"
    state: absent
  failed_when: false
  when: ai_benchmark_force_unlock | default(false) | bool
  tags: cleanup

- name: Check for ongoing benchmark processes
  ansible.builtin.shell: |
    pgrep -f "python.*milvus_benchmark\.py" | xargs -r ps -p 2>/dev/null | grep -v "sh -c" | grep -v grep | wc -l || echo "0"
  register: benchmark_check
  changed_when: false
  failed_when: false

- name: Fail if benchmark is already running
  ansible.builtin.fail:
    msg: |
      ERROR: A benchmark is already running on this system!
      Number of benchmark processes: {{ benchmark_check.stdout }}
      Please wait for the current benchmark to complete or terminate it before starting a new one.
  when: benchmark_check.stdout | int > 0

- name: Ensure benchmark results directory exists
  ansible.builtin.file:
    path: "{{ ai_benchmark_results_dir }}"
    state: directory
    mode: '0755'

- name: Check for benchmark lock file
  ansible.builtin.stat:
    path: "{{ ai_benchmark_results_dir }}/.benchmark.lock"
  register: lock_file

- name: Check if lock file is stale (older than 5 minutes)
  ansible.builtin.set_fact:
    lock_is_stale: "{{ (ansible_date_time.epoch | int - lock_file.stat.mtime | default(0) | int) > 300 }}"
  when: lock_file.stat.exists

- name: Remove stale lock file
  ansible.builtin.file:
    path: "{{ ai_benchmark_results_dir }}/.benchmark.lock"
    state: absent
  when:
    - lock_file.stat.exists
    - lock_is_stale|default(false)|bool

- name: Fail if recent benchmark lock exists
  ansible.builtin.fail:
    msg: |
      ERROR: Benchmark lock file exists at {{ ai_benchmark_results_dir }}/.benchmark.lock
      This indicates a benchmark may be in progress or was terminated abnormally.
      Lock file age: {{ (ansible_date_time.epoch | int - lock_file.stat.mtime | default(0) | int) }} seconds
      If you're sure no benchmark is running, remove the lock file manually.
  when:
    - lock_file.stat.exists
    - not lock_is_stale|default(false)|bool

- name: Run benchmark with lock management
  block:
    - name: Create benchmark lock file
      ansible.builtin.file:
        path: "{{ ai_benchmark_results_dir }}/.benchmark.lock"
        state: touch
        mode: '0644'
      register: lock_created

    - name: Create benchmark working directory
      ansible.builtin.file:
        path: "{{ ai_benchmark_results_dir }}/workdir"
        state: directory
        mode: '0755'

    - name: Copy benchmark script
      ansible.builtin.copy:
        src: milvus_benchmark.py
        dest: "{{ ai_benchmark_results_dir }}/workdir/milvus_benchmark.py"
        mode: '0755'

    - name: Ensure Python venv package is installed
      ansible.builtin.package:
        name:
          - python3-venv
          - python3-pip
          - python3-dev
        state: present
      become: true

    - name: Clean up any globally installed packages (if accidentally installed)
      ansible.builtin.shell: |
        pip3 uninstall -y pymilvus numpy 2>/dev/null || true
      become: true
      changed_when: false
      failed_when: false

    - name: Check if virtual environment exists
      ansible.builtin.stat:
        path: "{{ ai_benchmark_results_dir }}/venv/bin/python"
      register: venv_exists

    - name: Verify virtual environment has required packages
      block:
        - name: Check if pymilvus is installed in virtual environment
          ansible.builtin.command: "{{ ai_benchmark_results_dir }}/venv/bin/python -c 'import pymilvus; print(pymilvus.__version__)'"
          register: pymilvus_check
          changed_when: false
          failed_when: false

        - name: Display current pymilvus version
          ansible.builtin.debug:
            msg: "Current pymilvus version: {{ pymilvus_check.stdout }}"
          when: pymilvus_check.rc == 0

        - name: Virtual environment is not properly configured
          ansible.builtin.debug:
            msg: "Virtual environment at {{ ai_benchmark_results_dir }}/venv is missing or incomplete. Please run 'make ai' first to set up the environment."
          when: not venv_exists.stat.exists or pymilvus_check.rc != 0

        - name: Fail if virtual environment is not ready
          ansible.builtin.fail:
            msg: "Virtual environment is not properly configured. Please run 'make ai' to set up the environment first."
          when: not venv_exists.stat.exists or pymilvus_check.rc != 0

    - name: List installed packages in virtual environment for verification
      ansible.builtin.command: "{{ ai_benchmark_results_dir }}/venv/bin/pip list"
      register: pip_list
      changed_when: false

    - name: Display installed packages
      ansible.builtin.debug:
        msg: "Installed packages in venv: {{ pip_list.stdout }}"

    - name: Generate benchmark configuration
      ansible.builtin.template:
        src: benchmark_config.json.j2
        dest: "{{ ai_benchmark_results_dir }}/workdir/benchmark_config.json"
        mode: '0644'

    - name: Wait for Milvus to be ready
      ansible.builtin.wait_for:
        host: "localhost"
        port: "{{ ai_vector_db_milvus_port }}"
        delay: 10
        timeout: 300

    - name: Run Milvus benchmark for iteration {{ item }}
      ansible.builtin.command: >
        {{ ai_benchmark_results_dir }}/venv/bin/python
        {{ ai_benchmark_results_dir }}/workdir/milvus_benchmark.py
        --config {{ ai_benchmark_results_dir }}/workdir/benchmark_config.json
        --output {{ ai_benchmark_results_dir }}/results_{{ ansible_hostname }}_{{ item }}.json
      register: benchmark_result
      with_sequence: start=1 end={{ ai_benchmark_iterations }}
      tags: run_benchmark

    - name: Display benchmark results
      ansible.builtin.debug:
        var: benchmark_result
      when: benchmark_result is defined

  always:
    - name: Remove benchmark lock file
      ansible.builtin.file:
        path: "{{ ai_benchmark_results_dir }}/.benchmark.lock"
        state: absent
      failed_when: false
      when: lock_created is defined and lock_created.changed

    - name: Ensure lock file is removed (fallback)
      ansible.builtin.shell: rm -f {{ ai_benchmark_results_dir }}/.benchmark.lock
      failed_when: false
      when: lock_created is defined
