---
# First ensure we have the data partition for vLLM storage
- ansible.builtin.include_role:
    name: create_data_partition
  tags: ["data_partition", "vllm-storage"]

# Set up Docker mirror 9P mount if available and configured
- ansible.builtin.import_role:
    name: docker_mirror_9p
  tags: ["deps", "docker-config"]

- name: Set vLLM workflow variables
  set_fact:
    vllm_workflow_enabled: true
  tags: vars

- name: Install vLLM dependencies
  ansible.builtin.import_tasks: tasks/install-deps/main.yml
  tags: ["vllm", "deps"]

# Configure Docker and storage to use /data partition BEFORE starting any containers
# Only needed for Kubernetes-based deployments (docker and production-stack)
- name: Configure Docker to use /data for storage
  ansible.builtin.include_tasks: tasks/configure-docker-data.yml
  when: vllm_deployment_type | default('docker') in ['docker', 'production-stack']
  tags: ["deps", "docker-config", "storage", "vllm-deploy"]

# Route to appropriate deployment method based on configuration
- name: Deploy vLLM using latest Docker images
  ansible.builtin.include_tasks:
    file: tasks/deploy-docker.yml
    apply:
      tags: ["vllm-deploy"]
  when: vllm_deployment_type | default('docker') == 'docker'
  tags: ["vllm-deploy"]

- name: Deploy vLLM Production Stack with Helm
  ansible.builtin.include_tasks:
    file: tasks/deploy-production-stack.yml
    apply:
      tags: ["vllm-deploy"]
  when: vllm_deployment_type | default('docker') == 'production-stack'
  tags: ["vllm-deploy"]

- name: Deploy vLLM on bare metal
  ansible.builtin.include_tasks:
    file: tasks/deploy-bare-metal.yml
    apply:
      tags: ["vllm-deploy"]
  when: vllm_deployment_type | default('docker') == 'bare-metal'
  tags: ["vllm-deploy"]

- name: vLLM benchmark tasks
  tags: vllm-benchmark
  when: vllm_benchmark_enabled | default(true)
  block:
    - name: Create benchmark script
      template:
        src: vllm-benchmark.py.j2
        dest: "{{ vllm_local_path }}/benchmark.py"
        mode: '0755'

    - name: Set up port forwarding for benchmarking
      when:
        - vllm_deployment_type | default('docker') != 'bare-metal'
        - vllm_k8s_type | default('minikube') == 'minikube'
      become: no
      ansible.builtin.command:
        cmd: kubectl port-forward -n {{ vllm_helm_namespace | default('vllm-system') }} svc/vllm-service {{ vllm_api_port | default(8000) }}:8000
      async: 300
      poll: 0
      register: port_forward_task

    - name: Wait for port forwarding to be ready
      when:
        - vllm_deployment_type | default('docker') != 'bare-metal'
        - vllm_k8s_type | default('minikube') == 'minikube'
      ansible.builtin.wait_for:
        port: "{{ vllm_api_port | default(8000) }}"
        host: localhost
        delay: 2
        timeout: 30

    - name: Run benchmark
      become: no
      ansible.builtin.command:
        cmd: python3 benchmark.py
        chdir: "{{ vllm_local_path }}"
      register: benchmark_output
      ignore_errors: yes

    - name: Stop port forwarding
      when:
        - vllm_deployment_type | default('docker') != 'bare-metal'
        - vllm_k8s_type | default('minikube') == 'minikube'
        - port_forward_task is defined
      become: no
      ansible.builtin.async_status:
        jid: "{{ port_forward_task.ansible_job_id }}"
      register: job_result
      failed_when: false

    - name: Kill port forwarding if still running
      when:
        - vllm_deployment_type | default('docker') != 'bare-metal'
        - vllm_k8s_type | default('minikube') == 'minikube'
        - port_forward_task is defined
        - job_result.finished is defined
        - not job_result.finished
      become: no
      ansible.builtin.command:
        cmd: kill {{ port_forward_task.ansible_job_id }}
      ignore_errors: yes

    - name: Display benchmark results
      debug:
        msg: "{{ benchmark_output.stdout }}"
      when: benchmark_output.stdout is defined

    - name: Collect benchmark results from remote
      when: benchmark_output.rc == 0
      fetch:
        src: "{{ vllm_results_dir }}/benchmark_results.json"
        dest: "{{ topdir_path }}/workflows/vllm/results/{{ inventory_hostname }}_benchmark_results.json"
        flat: yes
      ignore_errors: yes

    - name: Collect system information
      ansible.builtin.setup:
        gather_subset:
          - hardware
          - virtual
      register: system_info

    - name: Save system information
      copy:
        content: |
          {
            "hostname": "{{ inventory_hostname }}",
            "distribution": "{{ ansible_distribution }}",
            "distribution_version": "{{ ansible_distribution_version }}",
            "kernel": "{{ ansible_kernel }}",
            "processor_count": {{ ansible_processor_count }},
            "processor_cores": {{ ansible_processor_cores }},
            "memtotal_mb": {{ ansible_memtotal_mb }},
            "virtualization_type": "{{ ansible_virtualization_type | default('bare-metal') }}",
            "virtualization_role": "{{ ansible_virtualization_role | default('host') }}",
            "date": "{{ ansible_date_time.iso8601 }}"
          }
        dest: "{{ vllm_results_dir }}/system_info.json"
        mode: '0644'

    - name: Collect system information to control host
      fetch:
        src: "{{ vllm_results_dir }}/system_info.json"
        dest: "{{ topdir_path }}/workflows/vllm/results/{{ inventory_hostname }}_system_info.json"
        flat: yes

- name: vLLM monitoring tasks
  tags: vllm-monitor
  when:
    - vllm_observability_enabled | default(true)
    - vllm_deployment_type | default('docker') != 'bare-metal'
  block:
    - name: Get Grafana service information
      become: no
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Service
        namespace: "{{ vllm_helm_namespace | default('vllm-system') }}"
        name: vllm-grafana
      register: grafana_service

    - name: Get Prometheus service information
      become: no
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Service
        namespace: "{{ vllm_helm_namespace | default('vllm-system') }}"
        name: vllm-prometheus
      register: prometheus_service

    - name: Display monitoring URLs
      debug:
        msg: |
          Monitoring Stack URLs:
          {% if vllm_k8s_type | default('minikube') == 'minikube' %}
          Grafana: kubectl port-forward -n {{ vllm_helm_namespace | default('vllm-system') }} svc/vllm-grafana {{ vllm_grafana_port | default(3000) }}:3000
          Prometheus: kubectl port-forward -n {{ vllm_helm_namespace | default('vllm-system') }} svc/vllm-prometheus {{ vllm_prometheus_port | default(9090) }}:9090
          {% else %}
          Grafana: http://{{ grafana_service.resources[0].status.loadBalancer.ingress[0].ip | default('pending') }}:{{ vllm_grafana_port | default(3000) }}
          Prometheus: http://{{ prometheus_service.resources[0].status.loadBalancer.ingress[0].ip | default('pending') }}:{{ vllm_prometheus_port | default(9090) }}
          {% endif %}

- name: vLLM bare-metal monitoring info
  tags: vllm-monitor
  when: vllm_deployment_type | default('docker') == 'bare-metal'
  debug:
    msg: |
      Bare-metal deployment does not include Grafana/Prometheus monitoring stack.
      vLLM service logs available via: sudo journalctl -u {{ vllm_bare_metal_service_name | default('vllm') }} -f

- name: vLLM cleanup for bare metal
  ansible.builtin.include_tasks: tasks/cleanup-bare-metal.yml
  when: vllm_deployment_type | default('docker') == 'bare-metal'
  tags: ["vllm-cleanup"]

- name: vLLM cleanup tasks (Kubernetes)
  tags: vllm-cleanup
  when: vllm_deployment_type | default('docker') != 'bare-metal'
  block:
    - name: Delete all resources in vLLM namespace
      become: no
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ vllm_helm_namespace | default('vllm-system') }}"
        state: absent
        wait: true
      ignore_errors: yes

    - name: Delete Helm release if exists
      become: no
      kubernetes.core.helm:
        name: "{{ vllm_helm_release_name | default('vllm') }}"
        namespace: "{{ vllm_helm_namespace | default('vllm-system') }}"
        state: absent
      ignore_errors: yes

- name: vLLM teardown tasks
  tags: vllm-teardown
  block:
    - name: Delete vLLM deployment
      become: no
      ansible.builtin.command:
        cmd: kubectl delete -f {{ vllm_local_path }}/vllm-deployment.yaml
      ignore_errors: yes

    - name: Delete namespace
      become: no
      kubernetes.core.k8s:
        name: "{{ vllm_helm_namespace | default('vllm-system') }}"
        api_version: v1
        kind: Namespace
        state: absent
        wait: true

    - name: Stop Minikube if configured
      when: vllm_k8s_type | default('minikube') == 'minikube'
      become: no
      ansible.builtin.command:
        cmd: minikube stop
      ignore_errors: yes

- name: vLLM results tasks
  tags: vllm-results
  block:
    - name: Check if benchmark results exist
      stat:
        path: "{{ vllm_results_dir }}/benchmark_results.json"
      register: results_file

    - name: Collect benchmark results from remote to control host
      when: results_file.stat.exists
      fetch:
        src: "{{ vllm_results_dir }}/benchmark_results.json"
        dest: "{{ topdir_path }}/workflows/vllm/results/{{ inventory_hostname }}_benchmark_results.json"
        flat: yes
      ignore_errors: yes

    - name: Check if system info exists
      stat:
        path: "{{ vllm_results_dir }}/system_info.json"
      register: sysinfo_file

    - name: Collect system information to control host
      when: sysinfo_file.stat.exists
      fetch:
        src: "{{ vllm_results_dir }}/system_info.json"
        dest: "{{ topdir_path }}/workflows/vllm/results/{{ inventory_hostname }}_system_info.json"
        flat: yes
      ignore_errors: yes

    - name: Read benchmark results
      when: results_file.stat.exists
      slurp:
        src: "{{ vllm_results_dir }}/benchmark_results.json"
      register: benchmark_data

    - name: Display benchmark results
      when: results_file.stat.exists
      debug:
        msg: |
          === vLLM Benchmark Results ===
          {{ benchmark_data.content | b64decode | from_json | to_nice_yaml }}

    - name: No results found
      when: not results_file.stat.exists
      debug:
        msg: "No benchmark results found. Run 'make vllm-benchmark' first."

- name: vLLM visualization tasks
  tags: vllm-visualize
  block:
    - name: Create local results directory
      file:
        path: "{{ topdir_path }}/workflows/vllm/results/html"
        state: directory
        mode: '0755'
      delegate_to: localhost
      become: no
      run_once: true

    - name: Generate visualization script
      template:
        src: vllm-visualize.py.j2
        dest: "{{ topdir_path }}/workflows/vllm/results/visualize.py"
        mode: '0755'
      delegate_to: localhost
      become: no
      run_once: true

    - name: Check for collected results
      find:
        paths: "{{ topdir_path }}/workflows/vllm/results"
        patterns: "*_benchmark_results.json"
      register: result_files
      delegate_to: localhost
      become: no
      run_once: true

    - name: Generate HTML visualization
      when: result_files.files | length > 0
      ansible.builtin.command:
        cmd: python3 visualize.py
        chdir: "{{ topdir_path }}/workflows/vllm/results"
      delegate_to: localhost
      become: no
      run_once: true
      register: viz_output

    - name: Display visualization results
      when: result_files.files | length > 0
      debug:
        msg: |
          Visualization complete!
          Open the following file in your browser:
          {{ topdir_path }}/workflows/vllm/results/html/index.html

    - name: No results to visualize
      when: result_files.files | length == 0
      debug:
        msg: "No benchmark results found. Run 'make vllm-benchmark' first to generate data."
