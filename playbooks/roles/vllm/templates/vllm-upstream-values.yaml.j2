# vLLM Production Stack Helm values
# Generated by kdevops

# Router configuration
router:
  enabled: {{ vllm_router_enabled | default(true) | lower }}
  image:
    repository: "{{ vllm_prod_stack_router_image | default('ghcr.io/vllm-project/production-stack/router') }}"
    tag: "{{ vllm_prod_stack_router_tag | default('latest') }}"
  replicaCount: 1
  algorithm: "{{ vllm_router_algorithm | default('round_robin') }}"
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      cpu: 4
      memory: 8Gi

# vLLM Engine configuration
engine:
  replicaCount: {{ vllm_replica_count | default(1) }}
  image:
{% if vllm_use_cpu_inference | default(false) %}
    repository: substratusai/vllm
    tag: v0.6.3-cpu
{% else %}
    repository: vllm/vllm-openai
    tag: latest
{% endif %}

  model:
    name: "{{ vllm_model_name | default('opt-125m') }}"
    url: "{{ vllm_model_url | default('facebook/opt-125m') }}"
{% if vllm_hf_token is defined and vllm_hf_token %}
    hf_token: "{{ vllm_hf_token }}"
{% endif %}

  resources:
    requests:
      cpu: {{ vllm_request_cpu | default(8 if vllm_use_cpu_inference else 4) }}
      memory: "{{ vllm_request_memory | default('32Gi' if vllm_use_cpu_inference else '16Gi') }}"
{% if not vllm_use_cpu_inference | default(false) %}
      nvidia.com/gpu: {{ vllm_request_gpu | default(1) }}
{% endif %}
    limits:
      cpu: {{ vllm_request_cpu * 2 | default(16 if vllm_use_cpu_inference else 8) }}
      memory: "{{ vllm_request_memory | default('32Gi' if vllm_use_cpu_inference else '16Gi') }}"
{% if not vllm_use_cpu_inference | default(false) %}
      nvidia.com/gpu: {{ vllm_request_gpu | default(1) }}
{% endif %}

  vllmConfig:
    maxModelLen: {{ vllm_max_model_len | default(2048) }}
{% if vllm_use_cpu_inference | default(false) %}
    device: "cpu"
    dtype: "float32"
    tensorParallelSize: 1
{% else %}
    dtype: "{{ vllm_dtype | default('auto') }}"
    tensorParallelSize: {{ vllm_tensor_parallel_size | default(1) }}
    gpuMemoryUtilization: {{ vllm_gpu_memory_utilization | default('0.9') }}
{% endif %}
    enablePrefixCaching: {{ vllm_enable_prefix_caching | default(false) | lower }}
    enableChunkedPrefill: {{ vllm_enable_chunked_prefill | default(false) | lower }}

# LMCache configuration
{% if vllm_lmcache_enabled | default(false) %}
lmcache:
  enabled: true
  cpuOffloadingBufferSize: "{{ vllm_lmcache_cpu_buffer_size | default('30') }}"
{% else %}
lmcache:
  enabled: false
{% endif %}

# Monitoring configuration
{% if vllm_prod_stack_enable_monitoring | default(true) %}
monitoring:
  enabled: true
  prometheus:
    enabled: true
    retention: 7d
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 2
        memory: 4Gi

  grafana:
    enabled: true
    adminPassword: "{{ vllm_grafana_admin_password | default('admin') }}"
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1
        memory: 1Gi

    # Pre-configured dashboards
    dashboards:
      - vllm-overview
      - vllm-performance
      - vllm-requests
      - vllm-gpu-metrics
{% else %}
monitoring:
  enabled: false
{% endif %}

# Autoscaling configuration
{% if vllm_prod_stack_enable_autoscaling | default(false) %}
autoscaling:
  enabled: true
  minReplicas: {{ vllm_prod_stack_min_replicas | default(1) }}
  maxReplicas: {{ vllm_prod_stack_max_replicas | default(5) }}
  targetGPUUtilization: {{ vllm_prod_stack_target_gpu_utilization | default(80) }}
{% else %}
autoscaling:
  enabled: false
{% endif %}

# Service configuration
service:
  type: {{ 'LoadBalancer' if vllm_k8s_existing | default(false) else 'ClusterIP' }}
  port: {{ vllm_api_port | default(8000) }}
{% if vllm_api_key is defined and vllm_api_key %}
  apiKey: "{{ vllm_api_key }}"
{% endif %}

# Persistence
persistence:
  enabled: true
  storageClass: {{ vllm_storage_class | default('') }}
  modelCache:
    size: 100Gi
    path: /models

# Node affinity for GPU nodes
{% if not vllm_use_cpu_inference | default(false) %}
nodeSelector:
  nvidia.com/gpu: "true"

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
{% endif %}
