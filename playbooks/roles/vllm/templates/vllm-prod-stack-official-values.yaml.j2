# vLLM Production Stack Official Helm Chart values
# Generated by kdevops for github.com/vllm-project/production-stack

# Serving engine configuration
servingEngineSpec:
  enableEngine: true
  labels:
    environment: "vllm"
    release: "vllm"

  # Runtime configuration - leave empty to use default
  runtimeClassName: ""

  # Model specifications - array format required by official chart
  modelSpec:
    - name: "{{ vllm_model_name | default('opt-125m') }}"
      # Use CPU-specific image for CPU inference, GPU image otherwise
      # For CPU: openeuler/vllm-cpu:latest (pre-built CPU image)
      # For GPU: vllm/vllm-openai:v0.10.2 (official GPU image)
      # Uses Docker mirror when available for faster deployments
      repository: "{{ vllm_engine_image_final | default(vllm_engine_image_repo | default('openeuler/vllm-cpu' if vllm_use_cpu_inference else 'vllm/vllm-openai')) }}"
      tag: "{{ vllm_engine_image_tag | default('latest' if vllm_use_cpu_inference else 'v0.10.2') }}"
      modelURL: "{{ vllm_model_url | default('facebook/opt-125m') }}"
      replicaCount: {{ vllm_replica_count | default(2) }}

      # Resource requests - conservative for CPU inference to fit in available resources
      requestCPU: {{ vllm_request_cpu | default(16 if vllm_use_cpu_inference else 4) }}
      requestMemory: "{{ vllm_request_memory | default('16Gi' if vllm_use_cpu_inference else '16Gi') }}"
{% if not vllm_use_cpu_inference | default(false) %}
      requestGPU: {{ vllm_request_gpu | default(1) }}
{% if vllm_gpu_type | default('') %}
      requestGPUType: "{{ vllm_gpu_type }}"
{% endif %}
{% else %}
      requestGPU: 0
{% endif %}

      # Resource limits (optional, but recommended)
      limitCPU: {{ (vllm_request_cpu | default(16 if vllm_use_cpu_inference else 4)) * 1.5 | int }}
      limitMemory: "{{ vllm_limit_memory | default('24Gi' if vllm_use_cpu_inference else '20Gi') }}"

      # Storage configuration - disabled for minikube/testing environments
{% if vllm_enable_model_cache | default(true) and not (vllm_k8s_minikube | default(false)) %}
      pvcStorage: "{{ vllm_model_cache_size | default('50Gi') }}"
      pvcAccessMode: ["ReadWriteOnce"]
      storageClass: "{{ vllm_storage_class | default('') }}"
{% endif %}

      # vLLM specific configuration - optimized for CPU or GPU
      vllmConfig:
        maxModelLen: {{ vllm_max_model_len | default(2048) }}
{% if vllm_use_cpu_inference | default(false) %}
        # CPU-specific settings
        dtype: "float32"  # CPU requires float32
        device: "cpu"
        tensorParallelSize: 1  # CPU doesn't support tensor parallelism
{% else %}
        # GPU-specific settings
        dtype: "{{ vllm_dtype | default('auto') }}"
        tensorParallelSize: {{ vllm_tensor_parallel_size | default(1) }}
        gpuMemoryUtilization: {{ vllm_gpu_memory_utilization | default('0.9') }}
{% endif %}
        # Add extra arguments
        extraArgs:
          - "--disable-log-requests"
{% if vllm_use_cpu_inference | default(false) %}
          - "--device"
          - "cpu"
          - "--dtype"
          - "float32"
{% endif %}

{% if vllm_enable_lmcache | default(false) %}
      # LMCache configuration for KV cache offloading
      lmcacheConfig:
        enabled: true
        cpuOffloadingBufferSize: "{{ vllm_lmcache_buffer_size | default('30') }}"
{% endif %}

# Router configuration
routerSpec:
  enabled: {{ vllm_router_enabled | default(true) | lower }}
{% if vllm_router_enabled | default(true) %}
  labels:
    environment: "vllm"
    release: "vllm"

  # Use the official production stack router
  # Uses Docker mirror when available for faster deployments
  repository: "{{ vllm_router_image_final | default(vllm_prod_stack_router_image | default('ghcr.io/vllm-project/production-stack/router')) }}"
  tag: "{{ vllm_prod_stack_router_tag | default('latest') }}"
  replicaCount: {{ vllm_router_replica_count | default(1) }}

  # Router resources
  requestCPU: {{ vllm_router_request_cpu | default(2) }}
  requestMemory: "{{ vllm_router_request_memory | default('4Gi') }}"
  limitCPU: {{ vllm_router_limit_cpu | default(4) }}
  limitMemory: "{{ vllm_router_limit_memory | default('8Gi') }}"

  # Routing configuration
  algorithm: "{{ vllm_router_algorithm | default('round_robin') }}"
{% if vllm_router_session_affinity | default(false) %}
  sessionAffinity: true
  sessionAffinityTimeout: {{ vllm_router_session_timeout | default(3600) }}
{% endif %}
{% endif %}

# Service configuration
service:
  type: {{ vllm_service_type | default('ClusterIP') }}
  port: {{ vllm_api_port | default(8000) }}
{% if vllm_service_type | default('ClusterIP') == 'LoadBalancer' %}
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
{% endif %}

# Monitoring configuration (if supported by the chart)
{% if vllm_prod_stack_enable_monitoring | default(true) %}
monitoring:
  enabled: true
  prometheus:
    enabled: true
    retention: "{{ vllm_prometheus_retention | default('7d') }}"
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 2
        memory: 4Gi

  grafana:
    enabled: true
    adminPassword: "{{ vllm_grafana_admin_password | default('admin') }}"
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1
        memory: 1Gi
{% endif %}

# Autoscaling configuration
{% if vllm_prod_stack_enable_autoscaling | default(false) %}
autoscaling:
  enabled: true
  minReplicas: {{ vllm_prod_stack_min_replicas | default(1) }}
  maxReplicas: {{ vllm_prod_stack_max_replicas | default(5) }}
  targetCPUUtilizationPercentage: {{ vllm_prod_stack_target_cpu | default(80) }}
{% if not vllm_use_cpu_inference | default(false) %}
  targetGPUUtilizationPercentage: {{ vllm_prod_stack_target_gpu_utilization | default(80) }}
{% endif %}
{% endif %}
