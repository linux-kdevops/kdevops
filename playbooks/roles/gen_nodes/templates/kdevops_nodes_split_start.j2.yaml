---
vagrant_global:
  box: "{{ vagrant_box }}"
  box_version: "{{ vagrant_box_version }}"
  memory: {{ libvirt_mem_mb }}
  cpus: {{ libvirt_vcpus_count }}
{% if nvme_zone_enable %}
  enable_zns: True
{% endif %}
  storage_pool_path: "{{ kdevops_storage_pool_path }}"
{% if virtualbox_provider %}
  virtualbox_cfg:
    auto_update: false
    enabled: "true"
    enable_sse4: "true"
    # To stress test a virtual NVMe controller you could peg all disks onto
    # one controller. We want to avoid this as our focus is testing filesystems
    # and not storage controllers however VirtualBox currently only supports
    # one NVMe storage controller. Set this to true only if you are adding
    # support for this upstream to VirtualBox.
    nvme_controller_per_disk: false
{% endif %}
{% if libvirt_provider %}
  libvirt_cfg:
{% if libvirt_override_machine_type %}
    machine_type: '{{ libvirt_machine_type }}'
{% endif %}
    # This lets the Ansible role kdevops_vagrant try to infer your default
    # distro group to use for QEMU. OpenSUSE and Fedora uses QEMU here by
    # default, debian uses libvirt-qemu. You can always override with the
    # environment variable KDEVOPS_VAGRANT_QEMU_GROUP. If not sure edit
    # /etc/libvirt/qemu.conf user and group settings. If using AppArmor /
    # SELinux you may run into snags, but that is out of scope of this project.
    qemu_group_auto: true
    qemu_group: 'libvirt-qemu'
    emulator_path: '{{ qemu_bin_path }}'
    uri: '{{ libvirt_uri }}'
    system_uri: '{{ libvirt_system_uri }}'
{% if libvirt_session %}
    session_socket: '{{ libvirt_session_socket }}'
    session_management_network_device: '{{ libvirt_session_management_network_device }}'
    session_public_network_dev: '{{ libvirt_session_public_network_dev }}'
{% endif %}
{% endif %}
  # This ends up slightly different depending on the vagrant provider right now.
  # For VirtualBox: /dev/nvme0n1, /dev/nvme0n2, etc.
  # For libvirt:    /dev/nvme0n1, /dev/nvme1n1, etc.
  # This is due to how VirtualBox only supports one NVMe storage controller
{% if libvirt_extra_storage_drive_ide %}
  extra_disks:
    data:
      size: 102400
    scratch:
      size: 102400
    extra1:
      size: 102400
    extra2:
      size: 102400
{% elif libvirt_extra_storage_drive_virtio %}
  extra_disks:
    data:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_virtio_physical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_virtio_logical_block_size }}
    scratch:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_virtio_physical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_virtio_logical_block_size }}
{% if not libvirt_largeio_enable %}
    extra1:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_virtio_physical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_virtio_logical_block_size }}
    extra2:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_virtio_physical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_virtio_logical_block_size }}
{% endif %}
{% elif libvirt_extra_storage_drive_scsi %}
  extra_disks:
    data:
      size: 102400
    scratch:
      size: 102400
    extra1:
      size: 102400
    extra2:
      size: 102400
{% elif libvirt_extra_storage_drive_nvme %}
  extra_disks:
    data:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
    scratch:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
{% if not libvirt_largeio_enable %}
    extra1:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
    extra2:
      size: 102400
      physical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
      logical_block_size: {{ libvirt_extra_storage_nvme_logical_block_size }}
{% endif %}
{% endif %}

{% if nvme_zone_enable %}
{% for n in range(1,14) %}
    zone{{ n }}:
      size: {{ nvme_zone_drive_size }}
      zoned: true
      zone_zasl: {{ nvme_zone_zasl }}
      zone_size: '{{ nvme_zone_size }}'
      zone_capacity: {{ nvme_zone_capacity }}
      zone_max_active: {{ nvme_zone_max_active }}
      zone_max_open: {{ nvme_zone_max_open }}
      physical_block_size: {{ nvme_zone_physical_block_size }}
      logical_block_size: {{ nvme_zone_logical_block_size }}
{% endfor %}
{% for n in range(1,2) %}
    zonenptwo{{ n }}:
      size: 98304
      zoned: true
      zone_zasl: 0
      zone_size: 96M
      zone_capacity: 0
      zone_max_active: 0
      zone_max_open: 0
      physical_block_size: 4096
      logical_block_size: 4096
{% endfor %}
{% endif %}
{% if libvirt_largeio_enable %}
{% set ns = namespace(lbs_idx=1)  %}
{% set max_pbs = libvirt_largeio_logical_compat_size  * (2 ** libvirt_largeio_pow_limit) %}
{% for n in range(0,libvirt_largeio_pow_limit+1) %}
{% for x in range(0,libvirt_largeio_drives_per_space) %}
{% set ns2 = namespace(pbs=libvirt_largeio_logical_compat_size  * (2 ** n)) %}
{% set ns3 = namespace(pbs_next_two=ns2.pbs * (2*(x-1))) %}
{% if (ns2.pbs == 512 or ns2.pbs == 4096 or ns2.pbs >= 16384) and (ns3.pbs_next_two <= max_pbs) %}
    largio{{ ns.lbs_idx }}:
      largio: true
      size: {{ libvirt_largeio_base_size }}
      physical_block_size: {{ ns2.pbs }}
{% if libvirt_largeio_logical_compat %}
      logical_block_size: {{ libvirt_largeio_logical_compat_size }}
{% else %}
      logical_block_size: {{ ns2.pbs }}
{% endif %}
{% set ns.lbs_idx = ns.lbs_idx + 1 %}
{% endif %}
{% endfor %}
{% endfor %}
{% endif %}

# Note: vagrant is not a fan of hosts with underscores.
#
# Modify the hostname to include a purpose, and then extract it later with
# ansible, for instance with:
#
# section: "{{ ansible_hostname | regex_replace('kdevops-') | regex_replace('-dev') | regex_replace('-', '_') }}"
#
# So if you say embraced kdevops-pokeyman and kdevops-pokeyman-dev you'd end up
# getting in the section always as pokeyman. As is below, with the above
# ansible regex you'd get the digits.
vagrant_boxes:
{% include './templates/hosts.j2' %}
