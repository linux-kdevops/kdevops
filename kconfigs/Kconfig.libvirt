config LIBVIRT_INSTALL
	bool "Install libvirt"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the Ansible role which installs
	  libvirt for you will be run. The goal will be to ensure you have
	  libvirt installed and running.

config LIBVIRT_CONFIGURE
	bool "Configure libvirt so you spawn guests as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the Ansible role which configures
	  libvirt for you will be run. This typically just requires adding the
	  user to a specific set of groups. The user must log out and back
	  in again, to ensure the new group takes effect. The goal in the
	  configuration will be to ensure you can use libvirt to spawn guests
	  as a regular user. You are encouraged to say y here unless you know
	  what you are doing or you already know this works. If you are unsure,
	  the litmus test for this is if you can spawn libvirt VMs, on any
	  demo setup available.

config LIBVIRT_VERIFY
	bool "Verify that a user can spawn libvirt as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  To enable a user to be able to spawn libvirt guests as a regular user
	  a user is typically added to a few groups. These groups are not
	  effective immediately, and so before a user can assume that they
	  use libvirt they must verify that the required groups are effective.
	  If you enable this option, we will spawn an Ansible role that will
	  verify and ensure that your user is already part of these groups.
	  You can safely say yes here.

choice
	prompt "Libvirt URI"
	default LIBVIRT_URI_SYSTEM if !DISTRO_FEDORA
	default LIBVIRT_URI_SESSION if DISTRO_FEDORA

config LIBVIRT_URI_SYSTEM
	bool "Use qemu:///system for the URI"
	output yaml
	help
	  The first design behind libvirt is to use the system URI, that is,
	  qemu:///system. All 'system' URIs (be it QEMU, lxc, uml, ...)
	  connect to the libvirtd daemon running as root which is launched at
	  system startup. Virtual machines created and run using 'system'
	  are usually launched as root, unless configured otherwise (for
	  example in /etc/libvirt/qemu.conf). A distribution can however still
	  allow users to use the system URI if they are added to the respective
	  groups to use libvirt, and this is the approach taken by kdevops when
	  this option is enabled.

	  You will definitely want to use qemu:///system if your VMs are
	  acting as servers. VM autostart on host boot only works for 'system',
	  and the root libvirtd instance has necessary permissions to use
	  proper networkings via bridges or virtual networks. qemu:///system
	  is generally what tools like virt-manager default to.

	  When this option is enabled libvirt's default built-in
	  URI is used along with the default network management interface,
	  libvirt socket, and the network interface assumed for bridging.

	  For more details on this refer to the libvirt wiki which still
	  advises in favor of the system URI over the session URI:

	  https://wiki.libvirt.org/page/FAQ#What_is_the_difference_between_qemu:.2F.2F.2Fsystem_and_qemu:.2F.2F.2Fsession.3F_Which_one_should_I_use.3F

config LIBVIRT_URI_SESSION
	bool "Use qemu:///session for the URI"
	help
	  A second design consideration has been implemented into libvirt to
	  enable users to use libvirt without the libvirt daemon needing to
	  run as root. All 'session' URIs launch a libvirtd instance as your
	  local user, and all VMs are run with local user permissions.

	  The benefit of qemu:///session is that permission issues vanish:
	  disk images can easily be stored in $HOME, serial PTYs are owned by
	  the user, etc.

	  qemu:///session has a serious drawback: since the libvirtd instance
	  does not have sufficient privileges, the only out of the box network
	  option is QEMU's usermode networking, which has non obvious
	  limitations, so its usage is discouraged. More info on QEMU
	  networking options: http://people.gnome.org/~markmc/qemu-networking.html
	  With regards to kdevops, if you use the session URI we don't
	  instantiate secondary interfaces with private IP addresses. This is
	  not a requirement for the currently supported workflows but if
	  you are doing custom networking stuff this may be more relevant for
	  you. Fedora defaults to the session URI.

	  When this option is enabled we modify libvirt's default
	  built-in URI for the session URI, and we also modify the default
	  network management interface to be virbr0, the default socket
	  is assumed to be /run/libvirt/libvirt-sock-ro. New Kconfig options
	  can be added later to customize those further if we really need
	  to.

	  Please note that sensible defaults are enabled for your Linux
	  distribution, so if your distribution does not have session URI
	  set by default it means it doesn't support it yet and you should
	  expect things to not work, and put the work to fix / enhance that
	  somehow. That work likely is not on kdevops... but perhaps this
	  could be wrong. Testing has be done with session support on Debian
	  testing, Ubuntu 21.10 and they both have issues. Don't enable session
	  support manually unless you know what you are doing.

config LIBVIRT_URI_CUSTOM
	bool "Custom QEMU URI"
	help
	  Select this option if you want to manually specify which URI to use.
	  In other words you know what you are doing.

endchoice

config LIBVIRT_URI_PATH
	string "Libvirt QEMU URI to use"
	default "qemu:///system" if LIBVIRT_URI_SYSTEM || LIBVIRT_URI_CUSTOM
	default "qemu:///session" if LIBVIRT_URI_SESSION
	help
	  By default libvirt uses a qemu:///system URI which assumes the libvirt
	  daemon runs as a user other than the user which is running the kdevops
	  commands. Libvirt has support for running the libvirt daemon as other
	  users using session support. This will be modified to a session URI
	  if you enable LIBVIRT_URI_SESSION. You can however set this to
	  something different to suit your exact needs here. This is the value
	  passed to the libvirt plugin. You should not have
	  to modify this value if you selected LIBVIRT_URI_SYSTEM or
	  LIBVIRT_URI_SESSION and are starting from a fresh 'make mrproper'
	  setting on kdevops, the appropriate value will be set for you.
	  You should only have to modify this manually if you set
	  LIBVIRT_URI_CUSTOM and you know what you are doing.

config LIBVIRT_SYSTEM_URI_PATH
	string "Libvirt system QEMU URI to use"
	default "qemu:///system"
	help
	  This is the URI of QEMU system connection, used to obtain the IP
	  address for management. This is used for the libvirt plugin
	  system_uri setting. If for whatever reason this needs to
	  be modified you can do so here. Even if you are using session
	  support you should leave this with the default qemu:///system setting
	  as this is still used to ensure your guest's IP address will be
	  communicated back to kdevops so it determines the guest is up and
	  you can ssh to it. Setting this to qemu:///session still gets the
	  guest up but kdevops won't know the guest is up, even though the
	  host can ssh to the guest. You should only modify this value if
	  you know what you are doing.

config LIBVIRT_QEMU_GROUP
	string
	default "qemu" if !DISTRO_DEBIAN && !DISTRO_UBUNTU
	default "libvirt-qemu" if DISTRO_DEBIAN || DISTRO_UBUNTU


config LIBVIRT_STORAGE_POOL_PATH
	string
	output yaml
	default LIBVIRT_STORAGE_POOL_PATH_AUTO if LIBVIRT && !LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	default LIBVIRT_STORAGE_POOL_PATH_AUTO if LIBVIRT && LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM if LIBVIRT && LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL

config QEMU_BIN_PATH
	string
	default QEMU_BIN_PATH_LIBVIRT if LIBVIRT
	default $(shell,./scripts/get_libvirt_qemu_bin_path.sh) if !QEMU_USE_DEVELOPMENT_VERSION

config LIBVIRT_URI
	string
	default "qemu:///system" if !LIBVIRT
	default LIBVIRT_URI_PATH if LIBVIRT

config LIBVIRT_SYSTEM_URI
	string
	default "qemu:///system" if !LIBVIRT
	default LIBVIRT_SYSTEM_URI_PATH if LIBVIRT

config LIBVIRT_SESSION
	bool
	default LIBVIRT_URI_SESSION

# Only fedora is using this for now. We can add options to modify
# this once this changes or if someone wants to really modify these.
if LIBVIRT_SESSION

config LIBVIRT_SESSION_SOCKET
	string
	default "/run/libvirt/libvirt-sock-ro"

config LIBVIRT_SESSION_MANAGEMENT_NETWORK_DEVICE
	string
	default "virbr0"

config LIBVIRT_SESSION_PUBLIC_NETWORK_DEV
	string
	default "virbr0"

endif # LIBVIRT_SESSION

config USE_LIBVIRT_MIRROR
	bool
	default y if USE_LOCAL_LINUX_MIRROR
	default n if !USE_LOCAL_LINUX_MIRROR

config QEMU_BUILD
	bool "Should we build QEMU for you?"
	select NEEDS_LOCAL_DEVELOPMENT_PATH
	help
	  You only want to enable this option if your distribution package
	  of QEMU does not have support for the features you need. For
	  example this may be useful if you are a QEMU developer or are
	  relying on technology is still under development or if you have
	  a custom QEMU git URL.

if !QEMU_BUILD

config QEMU_USE_DEVELOPMENT_VERSION
	bool "Should we look for a development version of QEMU?"
	help
	  You want to enable this option if for example the currently
	  available version of QEMU does not yet have support for the feature
	  you are going to be working on.

	  Say yes here if you are compiling your own version of QEMU.

endif # !QEMU_BUILD

if QEMU_BUILD

choice
	prompt "QEMU git URL to use"
	default QEMU_BUILD_JIC23

config QEMU_BUILD_UPSTREAM
	bool "https://gitlab.com/qemu-project/qemu.git"
	help
	  Select this option if you want to use the upstream QEMU git repo.

config QEMU_BUILD_JIC23
	bool "https://gitlab.com/jic23/qemu.git"
	help
	  Select this option if you want to use Cameron's QEMU git repo.
	  This has a few CXL bells and whistles which are not yet upstream.

config QEMU_BUILD_MANUAL
	bool "Custom QEMU git URL"
	help
	  Select this option if you want to specify your own git URL.

endchoice

config QEMU_BUILD_GIT
	string "Git tree for QEMU to clone on localhost"
	default "/mirror/qemu.git" if USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_UPSTREAM || QEMU_BUILD_MANUAL
	default "/mirror/qemu-jic23.git" if USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_JIC23
	default DEFAULT_QEMU_GITHUB_HTTPS_URL if !USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_UPSTREAM || QEMU_BUILD_MANUAL
	default DEFAULT_QEMU_JIC23_GITHUB_HTTPS_URL if !USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_JIC23
	help
	  This is the git URL to use to clone and then build QEMU for you on
	  your localhost.

config QEMU_BUILD_GIT_DATA_PATH
	string "The destination directory where to clone the QEMU git tree"
	default "{{local_dev_path}}/qemu"
	help
	  This is the target location of where to clone the above git tree.
	  Note that {{local_dev_path}} corresponds to the location set by the
	  configuration option CONFIG_NEEDS_LOCAL_DEVELOPMENT_PATH.

config QEMU_BUILD_GIT_VERSION
	string "The version of QEMU to build"
	default "cxl-2023-05-19" if QEMU_BUILD_JIC23
	default "v8.0.0-rc1" if QEMU_BUILD_UPSTREAM
	help
	  This is the target build version of QEMU to build. Please use
	  at least v7.2.0 for CXL support. v8.0.0-rc1 has some build fixes
	  for newer compilers so it is the default now.

config QEMU_USE_DEVELOPMENT_VERSION
	bool
	default y

endif # QEMU_BUILD

config QEMU_BIN_PATH_LIBVIRT
	string "QEMU binary path to use"
	default "/usr/local/bin/qemu-system-x86_64" if QEMU_USE_DEVELOPMENT_VERSION
	default "/usr/bin/qemu-system-x86_64" if !QEMU_USE_DEVELOPMENT_VERSION

config QEMU_INSTALL_DIR_LIBVIRT
	string "Path to install QEMU"
	default "/usr/local/bin" if QEMU_USE_DEVELOPMENT_VERSION
	default "/usr/bin" if !QEMU_USE_DEVELOPMENT_VERSION

config QEMU_VIRSH_CAN_SUDO
	bool
	default $(shell, ./scripts/get_libvirsh_can_sudo.sh)

config LIBVIRT_LARGE_CPU
	bool "Enable extremely large CPU count"
	depends on LIBVIRT
	help
	  Select this option if you want to enable larger CPUs
	  over what most KVM / libvirt configurations allow these days.
	  The maximum number of virtual CPUs you can use can vary
	  system to system, but if using KVM one can query KVM using
	  KVM_CHECK_EXTENSION ioctl to /dev/kvm and query for the
	  KVM_CAP_MAX_VCPUS. Even though this has been 1024 since
	  commit 074c82c8f7cf8a ("kvm: x86: Increase MAX_VCPUS to 1024")
	  and this was merged on v5.15 libvirt still uses only max
	  255. You can also enable an ioapic and iommu to bump you to
	  288, but these are software emulated and so are slow and
	  not recommended.

	  We could later add support to make dynconfig support here
	  to dynamically detect you CPU limits. Most distros can
	  use virsh maxvcpus to get the limit but we can later
	  consider just installing kvmtool as well. For now deal with
	  a max 255 by default as that seems to be what most rolling
	  Linux distributions are using...

	  References:

	  https://www.suse.com/support/kb/doc/?id=000019723
	  https://lwn.net/Articles/658511/
	  https://git.kernel.org/pub/scm/linux/kernel/git/will/kvmtool.git/

choice
	prompt "Guest vCPUs"
	default LIBVIRT_VCPUS_8

config LIBVIRT_VCPUS_2
	bool "2"
	help
	  Use 2 vCPUs on guests.

config LIBVIRT_VCPUS_4
	bool "4"
	help
	  Use 4 vCPUs on guests.

config LIBVIRT_VCPUS_8
	bool "8"
	help
	  Use 8 vCPUs on guests.

config LIBVIRT_VCPUS_16
	bool "16"
	help
	  Use 16 vCPUs on guests.

config LIBVIRT_VCPUS_32
	bool "32"
	help
	  Use 32 vCPUs on guests.

config LIBVIRT_VCPUS_64
	bool "64"
	help
	  Use 64 vCPUs on guests.

config LIBVIRT_VCPUS_128
	bool "128"
	help
	  Use 128 vCPUs on guests.

config LIBVIRT_VCPUS_255
	bool "255"
	help
	  Use 255 vCPUs on guests.

config LIBVIRT_VCPUS_288
	bool "288"
	depends on LIBVIRT_LARGE_CPU
	help
	  Use 288 vCPUs on guests.

config LIBVIRT_VCPUS_512
	bool "512"
	depends on LIBVIRT_LARGE_CPU
	help
	  Use 512 vCPUs on guests.

endchoice

config LIBVIRT_VCPUS_COUNT
	int
	default 2 if LIBVIRT_VCPUS_2
	default 4 if LIBVIRT_VCPUS_4
	default 8 if LIBVIRT_VCPUS_8
	default 16 if LIBVIRT_VCPUS_16
	default 32 if LIBVIRT_VCPUS_32
	default 64 if LIBVIRT_VCPUS_64
	default 128 if LIBVIRT_VCPUS_128
	default 255 if LIBVIRT_VCPUS_255
	default 288 if LIBVIRT_VCPUS_288
	default 512 if LIBVIRT_VCPUS_512
	help
	  The number of virtual CPUs to use per guest.

choice
	prompt "How much GiB memory to use per guest"
	default LIBVIRT_MEM_4G

config LIBVIRT_MEM_2G
	bool "2"
	help
	  Use 2 GiB of RAM on guests. Most workflows should work well
	  except for:
	  - xfs/074 is known to fail due to the amount of RAM used by the
	    obsolete xfs_scratch. Since xfs_scratch is obsolete this test
	    will not run by default unless the test runner uses
	    FORCE_XFS_CHECK_PROG=yes
	  - git cloning linux-next requires > 2 GiB RAM if your clone is not
	    shallow (BOOTLINUX_SHALLOW_CLONE)
	  - xfs/084 and generic/627 often get killed by OOM killer and skipped

config LIBVIRT_MEM_3G
	bool "3"
	help
	  Use 3 GiB of RAM on guests. No OOM killed tests observed when using
	  3 GiB of RAM on most workflows.

config LIBVIRT_MEM_4G
	bool "4"
	help
	  Use 4 GiB of RAM on guests. No known issues are known when using
	  4 GiB of RAM on most workflows.

config LIBVIRT_MEM_8G
	bool "8"
	help
	  Use 8 GiB of RAM on guests.

config LIBVIRT_MEM_16G
	bool "16"
	help
	  Use 16 GiB of RAM on guests.

config LIBVIRT_MEM_32G
	bool "32"
	help
	  Use 32 GiB of RAM on guests.

endchoice

config LIBVIRT_MEM_MB
	int
	default 2048 if LIBVIRT_MEM_2G
	default 3072 if LIBVIRT_MEM_3G
	default 4096 if LIBVIRT_MEM_4G
	default 8192 if LIBVIRT_MEM_8G
	default 16384 if LIBVIRT_MEM_16G
	default 32768 if LIBVIRT_MEM_32G
	help
	  How much MiB of RAM to use per guest.

config HAVE_LIBVIRT_PCIE_PASSTHROUGH
	bool
	default $(shell, scripts/check_pciepassthrough_kconfig.sh passthrough_libvirt.generated)


choice
	prompt "Machine type to use"
	default LIBVIRT_MACHINE_TYPE_Q35 if TARGET_ARCH_X86_64
	default LIBVIRT_MACHINE_TYPE_VIRT if TARGET_ARCH_ARM64

config LIBVIRT_MACHINE_TYPE_Q35
	bool "q35"
	depends on TARGET_ARCH_X86_64
	help
	  Use q35 for the machine type. This will be required for things like
	  CXL or PCIe passthrough.

config LIBVIRT_MACHINE_TYPE_VIRT
	bool "virt"
	depends on TARGET_ARCH_ARM64
	help
	  Use virt for the machine type. This is the default on aarch64 hosts.

endchoice

config LIBVIRT_HOST_PASSTHROUGH
	bool "Use CPU host-passthrough"
	default y
	help
	  Enable this to be able to also carry the same CPU your host has to
	  the guest. As per QEMU documentation this is the recommended CPU
	  type to use, provided live migration is not required. And we're
	  not supporting live-migration on kdevops so this our default too.
	  This will also enable you to use things like perf stat and
	  get access to some PMUs:

	  perf stat --repeat 2 -e \
	    dTLB-loads,dTLB-load-misses,dTLB-stores,dTLB-stores-misses,\
	    iTLB-loads,iTLB-load-misses,\
	    itlb_misses.walk_completed_4k\
	    itlb_misses.walk_completed_2m_4m\
	    page-faults,tlb:tlb_flush  \
	      --pre 'make -s mrproper defconfig' \
	    \-- make -s -j$(nproc) bzImage

choice
	prompt "Libvirt extra storage driver to use"
	default LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO

config LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	bool "NVMe"
	output yaml
	help
	  Use the QEMU NVMe driver for extra storage drives. We always expect
	  to use this as we expect *could* be outperforming the virtio driver.
	  Only if you enable this will you get support for ZNS too. We expect
	  the NVMe driver to always be the default. It also gives us parity with
	  cloud bringups.

config LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	bool "virtio"
	output yaml
	help
	  Use the QEMU virtio driver for extra storage drives. Use this if you
	  are having issues with "NVMe timeouts" issues when testing in a loop
	  with fstests and cannot upgrade your QEMU version. If you select this
	  you won't be able to test ZNS.

config LIBVIRT_EXTRA_STORAGE_DRIVE_IDE
	bool "ide"
	output yaml
	help
	  Use the QEMU ide driver for extra storage drives. This is useful for
	  really old Linux distributions that lack the virtio backend driver.

config LIBVIRT_EXTRA_STORAGE_DRIVE_SCSI
        bool "scsi"
        output yaml
        help
          Use the QEMU SCSI driver for extra storage drives. This relies on a
          virtio-scsi controller with scsi-hd devices attached.

endchoice

choice
	prompt "QEMU NVMe extra storage logical block size"
	default LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_512
	depends on LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	help
	  The logical block size to use for extra NVMe drives. This ends up
          what is put into the /sys/block/<disk>/queue/logical_block_size (and
	  /sys/block/<disk>/queue/physical_block_size) the smallest unit the
          storage device can address. It is typically 512 bytes.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_512
	bool "512 bytes"
	select EXTRA_STORAGE_SUPPORTS_512
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  512 bytes logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_1K
	bool "1 KiB"
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  1 KiB logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_2K
	bool "2 KiB"
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  2 KiB (2048 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_4K
	bool "4 KiB"
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  4 KiB (4096 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_8K
	bool "8 KiB"
	help
	  8 KiB (8192 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_16K
	bool "16 KiB"
	help
	  16 KiB (16384 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_32K
	bool "32 KiB"
	help
	  32 KiB (32768 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_64K
	bool "64 KiB"
	help
	  64 KiB (65536 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_128K
	bool "128 KiB"
	help
	  128 KiB (131072 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_256K
	bool "256 KiB"
	help
	  256 KiB (262144 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_512K
	bool "512 KiB"
	help
	  512 KiB (524288 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_1M
	bool "1 MiB"
	help
	  1 MiB (1048576 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_2M
	bool "2 MiB"
	help
	  2 MiB (2097152 bytes) logical block size.

endchoice

choice
	prompt "QEMU virtio extra storage physical block size"
	depends on LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	default LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_512
	help
	  The physical block size to use for extra drive. This ends up
	  what is put into the /sys/block/<disk>/queue/physical_block_size
	  and is the smallest unit a physical storage device can write
	  atomically.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_512
	bool "512 bytes"
	help
	  512 bytes logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_1K
	bool "1 KiB"
	help
	  1 KiB logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_2K
	bool "2 KiB"
	help
	  2 KiB (2048 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_4K
	bool "4 KiB"
	help
	  4 KiB (4096 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_8K
	bool "8 KiB"
	help
	  8 KiB (8192 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_16K
	bool "16 KiB"
	help
	  16 KiB (16384 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_32K
	bool "32 KiB"
	help
	  32 KiB (32768 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_64K
	bool "64 KiB"
	help
	  64 KiB (65536 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_128K
	bool "128 KiB"
	help
	  128 KiB (131072 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_256K
	bool "256 KiB"
	help
	  256 KiB (262144 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_512K
	bool "512 KiB"
	help
	  512 KiB (524288 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_1M
	bool "1 MiB"
	help
	  1 MiB (1048576 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_2M
	bool "2 MiB"
	help
	  2 MiB (2097152 bytes) logical block size.

endchoice

choice
	prompt "QEMU virtio extra storage logical block size"
	depends on LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	default LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_512
	help
	  The logical block size to use for extra drives. This ends up what is
	  put into the /sys/block/<disk>/queue/logical_block_size and the
	  smallest unit the storage device can address. It is typically 512
	  bytes.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_512
	bool "512 bytes"
	select EXTRA_STORAGE_SUPPORTS_512
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  512 bytes logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_1K
	bool "1 KiB"
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  1 KiB logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_2K
	bool "2 KiB"
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  2 KiB (2048 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_4K
	bool "4 KiB"
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  4 KiB (4096 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_8K
	bool "8 KiB"
	help
	  8 KiB (8192 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_16K
	bool "16 KiB"
	help
	  16 KiB (16384 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_32K
	bool "32 KiB"
	help
	  32 KiB (32768 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_64K
	bool "64 KiB"
	help
	  64 KiB (65536 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_128K
	bool "128 KiB"
	help
	  128 KiB (131072 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_256K
	bool "256 KiB"
	help
	  256 KiB (262144 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_512K
	bool "512 KiB"
	help
	  512 KiB (524288 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_1M
	bool "1 MiB"
	help
	  1 MiB (1048576 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_2M
	bool "2 MiB"
	help
	  2 MiB (2097152 bytes) logical block size.

endchoice

choice
	prompt "Libvirt aio mode"
	default LIBVIRT_AIO_MODE_IO_URING

config LIBVIRT_AIO_MODE_NATIVE
	bool "aio=native"
	depends on !KDEVOPS_LIBVIRT_PCIE_PASSTHROUGH
	help
	  Use the aio=native mode. For some older kernels it is known that
	  native will cause corruption if used on ext4 or xfs filesystem if
	  you also use cache=none. This corruption is documented for RHEL:

	  https://access.redhat.com/articles/41313
	  https://bugzilla.redhat.com/show_bug.cgi?id=615309

	  In terms of performance there are some recommendations to not use
	  aio=native on sparsefiles. The claim seems to be that a host
	  filesystems metadata write can block the AIO io_submit() call and
	  therefore block QEMU threads which expect AIO behaviour on the guest.
	  This is documented when on Openstack nova an aio mode option was
	  requested to be available for different backends:

	  If you want to use PCIe passthrough you cannot use aio=native.

	  https://review.opendev.org/c/openstack/nova-specs/+/232514/7/specs/mitaka/approved/libvirt-aio-mode.rst

config LIBVIRT_AIO_MODE_THREADS
	bool "aio=threads"
	help
	  Use the aio=threads mode. This might be more suitable for you if on
	  older kernels such as in RHEL6 and using sparsefiles and ext4 or xfs
	  on this host with cache=none.

config LIBVIRT_AIO_MODE_IO_URING
	bool "aio=io_uring"
	help
	  Use the aio=io_uring mode. This is currently experimental.
endchoice

config LIBVIRT_AIO_MODE
	string
	default "native" if LIBVIRT_AIO_MODE_NATIVE
	default "threads" if LIBVIRT_AIO_MODE_THREADS
	default "io_uring" if LIBVIRT_AIO_MODE_IO_URING

choice
	prompt "Libvirt cache mode"
	default LIBVIRT_AIO_CACHE_MODE_NONE

config LIBVIRT_AIO_CACHE_MODE_NONE
	bool "cache=none"
	help
	  Use the cache=none. IO from the guest is not cached on the host but
	  it may be kept in a writeback disk cache. This means that the actual
	  storage device may report a write as completed when the data is still
	  placed in the host's write queue only, the guest's virtual storage
	  adapter is informed that there is a writeback cache. So in essence
	  the guest's writes are directly accessing the host's disk. Use this
	  option for guests with large IO requirements. This is generally the
	  best option and is required for live migration.

config LIBVIRT_AIO_CACHE_MODE_WRITETHROUGH
	bool "cache=writethrough"
	help
	  cache=writethrough. IO from the guest is cached on the host but
	  written through to the physical medium. Writes are only reported as
	  completed when the data has been committed to the storage device. The
	  guest's virtual storage adapter is informed that there is no
	  writeback cache so the guest does not need to send flush commands
	  to manage integrity. This mode is slower and prone to scaling issues.
	  Best used for small number of guests with lower IO reqs. This should
	  be used on older guests which do not support writeback cache.

config LIBVIRT_AIO_CACHE_MODE_WRITEBACK
	bool "cache=writeback"
	help
	  cache=writeback. IO from the guest is cached on the host so it is
	  cached on the host's page cache. Writes are reported to the guest
	  when they are placed on the host's page cache. The guest's virtual
	  storage controller is informed of the writeback cache and therefore
	  expected to send flush commands as needed to manage data integrity.

config LIBVIRT_AIO_CACHE_MODE_DIRECTSYNC
	bool "cache=directsync"
	help
	  cache=directsync. Writes are reported only when the data has been
	  committed to the storage controller. The host cache is completely
	  bypassed. This mode is useful for guests which that do not send
	  flushes when needed.

config LIBVIRT_AIO_CACHE_MODE_UNSAFE
	bool "cache=unsafe"
	help
	  cache=unsafe. Similar to writeback except all of the flush commands
	  from the guest are ignored. This is useful if the one wants to
	  prioritize performance and one does not care about data loss.

endchoice

config LIBVIRT_AIO_CACHE_MODE
	string
	default "none" if LIBVIRT_AIO_CACHE_MODE_NONE
	default "writethrough" if LIBVIRT_AIO_CACHE_MODE_WRITETHROUGH
	default "writeback" if LIBVIRT_AIO_CACHE_MODE_WRITEBACK
	default "directsync" if LIBVIRT_AIO_CACHE_MODE_DIRECTSYNC
	default "unsafe" if LIBVIRT_AIO_CACHE_MODE_UNSAFE

choice
	prompt "Libvirt drive file format"
	depends on LIBVIRT
	default LIBVIRT_EXTRA_DRIVE_FORMAT_RAW

config LIBVIRT_EXTRA_DRIVE_FORMAT_QCOW2
	bool "Use qcow2 format"
	help
	  Select this option if you want to use the qcow2 file format for the
	  extra storage drives created for you. This is useful if you want to use
	  advanced features on the files such as growing them or freezing
	  them. There may be some odd issues however with very sub-optimal
	  features such as with discard, however these issues are still being
	  investigated.

config LIBVIRT_EXTRA_DRIVE_FORMAT_RAW
	bool "Use raw format"
	help
	  Select this option if you want to use the raw file format for the extra storage
	  drives created for you. One of the sweet spots for using raw and 4KiB
	  block sizes is the advantages of ensuring that when a filesystem
	  issues a punch hole through fallocate (FALLOC_FL_PUNCH_HOLE) write
	  zeroes (REQ_OP_WRITE_ZEROES) results in an actual deallocation of
	  blocks faster. When using qcow2 the default is to use a cluster of
	  64 KiB for blocks and by default blocks will only be marked
	  deallocated if a full cluster is zeroed or discarded. You can
	  fallocate (punch) holes on a filesystem with a smaller block
	  size than the default qcow2 cluster size (say 4 KiB), and so in
	  theory, this may cause undeterministic delays. In practice this may
	  end up in NVMe timeouts, but these issues are currently being
	  investigated, and if using the raw format proves to not cause NVMe
	  timeouts as observed with fstests punch tests while the backend
	  drive has low space (below 80%) this may become the default for
	  kdevops as it reflects a possible bug in QEMU.

endchoice

choice
	prompt "Libvirt storage pool path"
	default LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED if DISTRO_DEBIAN && QEMU_VIRSH_CAN_SUDO
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO if !DISTRO_SUSE && !DISTRO_DEBIAN
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	bool "Use an advanced smart inference for what storage pool path to use"
	help
	  If you are a power user of kdevops you likely want to enable this.
	  By default libvirt will assume that if you don't have a virsh pool
	  that the current directory will become the "default" storage pool
	  path. This is rather silly for an advanced setup. Consider a setup
	  where you have a set of different NVMe drivers mounted on different
	  partitions:

	  /dev/nvme0n1 --> /data1 with xfs
	  /dev/nvme1n1 --> /data2 with btrfs
	  /dev/nvme2n1 --> /data2 with ext4

	  In this setup, if you have a kdevops instance initialized in
	  /data1/ somewhere you likely want to use a virsh pool path under
	  /data1/libvirt/images/ so that all local requests to that drive
	  go to that storage pool that. By enabling this option kdevops will
	  infer this information for you and figure out what storage pool path
	  to use. It will scrape your existing virsh pool-list and use the first
	  path where the first directory of your current working directory
	  lies.

	  This is enabled by default on Debian only now as this has been
	  tested there first. If this gets tested on other distributions
	  they should migrate over to enable power users to be more lazy
	  on initial bringups.

	  Eventually this can be the default but it requires high confidence
	  that the heuristics will work on each distro with their own defaults.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current kdevops working directory"
	help
	  Select this option if you want to use the kdevops directory inside
	  where you git cloned kdevops as the libvirt storage pool path where
	  we'll download images and store images for guests spawned. If users
	  git cloned kdevops somewhere in their home directory they'll have to
	  make sure that the group which libvirt is configured to run for their
	  distribution can have access to that directory. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	bool "Use the same path typically used by each distribution"
	help
	  Select this option if you want to use the same location as the
	  distribution would typically use. We expect this to be
	  /var/lib/libvirt/images/ for most distributions, however we can
	  customize this further if this is not true by adding further checks.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the QEMU storage pool path. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config LIBVIRT_STORAGE_POOL_PATH_AUTO
	string
	default $(shell, ./scripts/get_libvirsh_pool_path.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default "/var/lib/libvirt/images" if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	default $(shell, scripts/cwd-append.sh kdevops) if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	help
	  The path to use for the libvirt storage pool path. This is also the path used to place the
	  additional NVMe drives created. kdevops adds a postfix "kdevops" to
	  this directory for full control over that
	  directory. For instance if this is /var/lib/libvirt/images/ kdevops
	  will store images in /var/lib/libvirt/images/ and
	  the NVMe qcow2 files created will go in by default to the directory
	  /var/lib/libvirt/images/kdevops/nvme_disks/guest-hostname/.

if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM
	string "Custom libvirt storage pool location"
	default "/var/lib/libvirt/images"
	help
	  The path to use for the libvirt storage pool path. This is also the path used to place the
	  additional NVMe drives created. kdevops adds a postfix "kdevops" to
	  this directory for full control over that
	  directory. For instance if this is /var/lib/libvirt/images/ kdevops
	  will store images in /var/lib/libvirt/images/ and
	  the NVMe qcow2 files created will go in by default to the directory
	  /var/lib/libvirt/images/kdevops/nvme_disks/guest-hostname/.

endif

config LIBVIRT_STORAGE_POOL_CREATE
	bool "Should we build a custom storage pool for you?"
	output yaml
	default n if !LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default $(shell, ./scripts/get_libvirsh_pool_enabled.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	help
	  By default libvirt assumes your storage pool name is "default" and
	  it expects libvirt to have created this for you. If you want to
	  use a custom pool name and path enable this. This is useful if
	  you want to place guest images on another path other than the
	  default libvirt has setup for you on the "default" pool name.

config LIBVIRT_STORAGE_POOL_NAME
	string "Libvirt storage pool name"
	output yaml
	depends on LIBVIRT_STORAGE_POOL_CREATE
	default "default" if !LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default $(shell, ./scripts/get_libvirsh_pool_name.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	help
	  The libvirt storage pool name to use. By default this is "default",
	  and this is typically defined by libvirt. Even if you create guests
	  on a separate directory unless you use a custom storage pool name
	  here the default path used where libvirt created the default
	  storage pool path will be used for data. You should modify the
	  storage pool name to something other than "default" here if you
	  are using a custom storage pool path and are doing so to ensure
	  you don't waste space in whatever path libvirt's default storage
	  pool path is set to. To check your libvirt's default storage pool
	  path you can run this and look for the path:

	    virsh pool-dumpxml default

	  You will need to use sudo on all distros which do not use
	  LIBVIRT_URI_SESSION (so all distros other than Fedora).
	  If you set here something other than 'default' kdevops will create
	  this pool upon 'make bringup' if the pool is not yet available. It
	  will do this with:

	    virsh pool-define-as NAME PATH
	    virsh pool-start NAME
	    virsh pool-autostart NAME

	  That name you use here is up to you but you should use something
	  which would make it obvious what it is for other users on the system.
	  For instance you may want to use a volume name of "data2" for a path
	  on a partition on /data2/ or something like that.

config LIBVIRT_ENABLE_GDB
    bool "Enable GDB debugging for the guest"
	output yaml
	help
	  Select this option if you want to enable debugging support for GDB.
	  By default , it is assumed that gdb is disabled since we don't want
	  to complicate this for the CI runs. If enabled then libvirt guest
	  xml for each guest will be configured to use gdb on a specific
	  tcp port.

config LIBVIRT_GDB_BASEPORT
    int
	default $(shell, ./scripts/get_gdb_base_port.sh) if LIBVIRT
	output yaml
	depends on LIBVIRT_ENABLE_GDB
	help
	  This option defines the base port to be used for the GDB.
	  Essentially we need to make QEMU listen for an incoming connection from
	  gdb on a TCP port. The default port is chosen to be 1234. However we
	  introduce variability for assigning the port to each guest by defining
	  a base port and adding an index to it based on the number of libvrt guest
	  nodes. Therefore the base port is extracted from the md5sum of the
	  /scripts/get_gdb_base_port.sh file and use the last 4 digits of the md5sum
	  of the file to assign to libvirt_gdb_baseport.

choice
	prompt "Storage pool user name to use"
	default KDEVOPS_STORAGE_POOL_USER_DEFAULT
	help
	  The storage pool user is name of the local user ID that has
	  access to the libvirt storage pool to be used by kdevops.

	  When there is only one user running kdevops on a system,
	  the default setting should work. To enable multiple users
	  to run kdevops on the same system, select
	  KDEVOPS_STORAGE_POOL_USER_AUTO.

config KDEVOPS_STORAGE_POOL_USER_DEFAULT
	bool "default"
	help
	  The default storage pool user name is always "kdevops".

config KDEVOPS_STORAGE_POOL_USER_AUTO
	bool "auto"
	help
	  Kdevops selects the storage pool user name.

config KDEVOPS_STORAGE_POOL_USER_CUSTOM
	bool "custom"
	help
	  Set a fixed custom storage pool user name.

endchoice

config KDEVOPS_STORAGE_POOL_USER_CUSTOM_NAME
	string "Storage pool user name"
	depends on KDEVOPS_STORAGE_POOL_USER_CUSTOM
	help
	  Set the name of the storage pool user.

config KDEVOPS_STORAGE_POOL_USER
	string
	output yaml
	default "kdevops" if KDEVOPS_STORAGE_POOL_USER_DEFAULT
	default $(shell, echo $USER) if KDEVOPS_STORAGE_POOL_USER_AUTO
	default KDEVOPS_STORAGE_POOL_USER_CUSTOM_NAME if KDEVOPS_STORAGE_POOL_USER_CUSTOM

config KDEVOPS_STORAGE_POOL_PATH
	string
	output yaml
	default "{{ libvirt_storage_pool_path }}/{{ kdevops_storage_pool_user }}"

source "kconfigs/Kconfig.libvirt.zns"
source "kconfigs/Kconfig.libvirt.largeio"
source "kconfigs/Kconfig.libvirt.cxl"
