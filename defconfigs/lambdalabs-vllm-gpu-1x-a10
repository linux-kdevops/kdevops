#
# Lambda Labs vLLM Production Stack - 1x A10 GPU ($0.75/hr)
#
# This combines:
#   - defconfigs/configs/lambdalabs-gpu-1x-a10.config (Terraform provisioning)
#   - defconfigs/configs/vllm-production-stack-gpu.config (vLLM deployment)
#
# Provisions a Lambda Labs GPU instance with NVIDIA A10 (24GB) and deploys
# the vLLM production stack for LLM inference workloads.
#
# ============================================================================
# NVIDIA GPU COMPATIBILITY (CUDA):
# ============================================================================
#
# vLLM v0.10.x uses FlashInfer CUDA kernels that require NVIDIA GPUs with
# compute capability >= 8.0. Older NVIDIA GPUs will fail with:
#   "RuntimeError: TopPSamplingFromProbs failed with error code
#    too many resources requested for launch"
#
# NVIDIA A10 Compatibility:
#   - Compute Capability: 8.6 âœ“ COMPATIBLE
#   - Memory: 24GB GDDR6
#   - Cost: $0.75/hour on Lambda Labs
#   - Perfect for: Production LLM inference, fine-tuning
#
# ============================================================================
# Usage:
#   make defconfig-lambdalabs-vllm-gpu-1x-a10
#   make bringup        # Provisions A10 GPU instance
#   make vllm           # Deploys vLLM production stack
#   make vllm-benchmark # Run performance benchmarks
# ============================================================================
#
# Lambda Labs GPU 1x A10 instance configuration
CONFIG_TERRAFORM=y
CONFIG_TERRAFORM_LAMBDALABS=y
CONFIG_TERRAFORM_LAMBDALABS_REGION_SMART_INFER=y
CONFIG_TERRAFORM_LAMBDALABS_INSTANCE_TYPE_GPU_1X_A10=y
CONFIG_TERRAFORM_SSH_CONFIG_GENKEY=y
CONFIG_TERRAFORM_SSH_CONFIG_GENKEY_OVERWRITE=y
CONFIG_TERRAFORM_SSH_CONFIG_GENKEY_EMPTY_PASSPHRASE=y

# vLLM Production Stack with GPU support
CONFIG_WORKFLOWS=y
CONFIG_WORKFLOWS_TESTS=y
CONFIG_WORKFLOWS_LINUX_TESTS=y
CONFIG_WORKFLOWS_DEDICATED_WORKFLOW=y
CONFIG_KDEVOPS_WORKFLOW_DEDICATE_VLLM=y

# vLLM Production Stack with Kubernetes
CONFIG_VLLM_PRODUCTION_STACK=y
CONFIG_VLLM_K8S_MINIKUBE=y
CONFIG_VLLM_VERSION_STABLE=y
CONFIG_VLLM_ENGINE_IMAGE_TAG="v0.10.2"
CONFIG_VLLM_HELM_RELEASE_NAME="vllm-prod"
CONFIG_VLLM_HELM_NAMESPACE="vllm-system"

# Production Stack components
CONFIG_VLLM_PROD_STACK_REPO="https://vllm-project.github.io/production-stack"
CONFIG_VLLM_PROD_STACK_CHART_VERSION="latest"
CONFIG_VLLM_PROD_STACK_ROUTER_IMAGE="ghcr.io/vllm-project/production-stack/router"
CONFIG_VLLM_PROD_STACK_ROUTER_TAG="latest"
CONFIG_VLLM_PROD_STACK_ENABLE_MONITORING=y
CONFIG_VLLM_PROD_STACK_ENABLE_AUTOSCALING=y
CONFIG_VLLM_PROD_STACK_MIN_REPLICAS=2
CONFIG_VLLM_PROD_STACK_MAX_REPLICAS=5
CONFIG_VLLM_PROD_STACK_TARGET_GPU_UTILIZATION=80

# Model configuration
CONFIG_VLLM_MODEL_URL="facebook/opt-125m"
CONFIG_VLLM_MODEL_NAME="opt-125m"

# GPU configuration - EXPLICITLY DISABLED CPU INFERENCE
# CONFIG_VLLM_USE_CPU_INFERENCE is not set
CONFIG_VLLM_REQUEST_GPU=1
CONFIG_VLLM_GPU_TYPE=""
CONFIG_VLLM_GPU_MEMORY_UTILIZATION="0.5"
CONFIG_VLLM_TENSOR_PARALLEL_SIZE=1

# Engine configuration for GPU
CONFIG_VLLM_REPLICA_COUNT=1
CONFIG_VLLM_REQUEST_CPU=8
CONFIG_VLLM_REQUEST_MEMORY="16Gi"
CONFIG_VLLM_MAX_MODEL_LEN=1024
CONFIG_VLLM_DTYPE="auto"

# Router and observability
CONFIG_VLLM_ROUTER_ENABLED=y
CONFIG_VLLM_ROUTER_ROUND_ROBIN=y
CONFIG_VLLM_OBSERVABILITY_ENABLED=y
CONFIG_VLLM_GRAFANA_PORT=3000
CONFIG_VLLM_PROMETHEUS_PORT=9090

# API configuration
CONFIG_VLLM_API_PORT=8000
CONFIG_VLLM_API_KEY=""
CONFIG_VLLM_HF_TOKEN=""

# Benchmarking
CONFIG_VLLM_BENCHMARK_ENABLED=y
CONFIG_VLLM_BENCHMARK_DURATION=60
CONFIG_VLLM_BENCHMARK_CONCURRENT_USERS=10
CONFIG_VLLM_BENCHMARK_RESULTS_DIR="/data/vllm-benchmark"
