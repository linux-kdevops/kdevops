#
# vLLM Production Stack with declared hosts - GPU ENABLED
#
# GPU COMPATIBILITY REQUIREMENTS:
#
# vLLM v0.10.x uses FlashInfer CUDA kernels that require GPUs with compute
# capability >= 8.0. GPUs with lower compute capability will fail with:
#   "RuntimeError: TopPSamplingFromProbs failed with error code
#    too many resources requested for launch"
#
# INCOMPATIBLE GPUs (compute capability < 8.0):
#   - Tesla T4 (7.5) - WILL NOT WORK with vLLM v0.10.x+
#   - Tesla V100 (7.0)
#   - Tesla P100 (6.0)
#   - GTX 1080 Ti (6.1)
#
# COMPATIBLE GPUs (compute capability >= 8.0):
#   - A100 (8.0)
#   - A10G (8.6)
#   - A30 (8.0)
#   - H100 (9.0)
#   - RTX 3090 (8.6)
#   - RTX 4090 (8.9)
#
# WORKAROUNDS for incompatible GPUs:
#   1. Use an older vLLM version (v0.6.x or earlier)
#   2. Use CPU inference mode (vllm-production-stack-declared-hosts)
#   3. Use a compatible GPU listed above
#
# Automatically generated file; DO NOT EDIT.
# kdevops 5.0.2 Configuration
#
CONFIG_WORKFLOWS=y
CONFIG_WORKFLOWS_TESTS=y
CONFIG_WORKFLOWS_LINUX_TESTS=y
CONFIG_WORKFLOWS_DEDICATED_WORKFLOW=y
CONFIG_KDEVOPS_WORKFLOW_DEDICATE_VLLM=y

# Skip bringup for declared hosts
CONFIG_SKIP_BRINGUP=y
CONFIG_KDEVOPS_USE_DECLARED_HOSTS=y

# vLLM Production Stack with Kubernetes on declared hosts
CONFIG_VLLM_PRODUCTION_STACK=y
CONFIG_VLLM_K8S_MINIKUBE=y
CONFIG_VLLM_VERSION_STABLE=y
CONFIG_VLLM_ENGINE_IMAGE_TAG="v0.10.2"
CONFIG_VLLM_HELM_RELEASE_NAME="vllm-prod"
CONFIG_VLLM_HELM_NAMESPACE="vllm-system"

# Production Stack components
CONFIG_VLLM_PROD_STACK_REPO="https://vllm-project.github.io/production-stack"
CONFIG_VLLM_PROD_STACK_CHART_VERSION="latest"
CONFIG_VLLM_PROD_STACK_ROUTER_IMAGE="ghcr.io/vllm-project/production-stack/router"
CONFIG_VLLM_PROD_STACK_ROUTER_TAG="latest"
CONFIG_VLLM_PROD_STACK_ENABLE_MONITORING=y
CONFIG_VLLM_PROD_STACK_ENABLE_AUTOSCALING=y
CONFIG_VLLM_PROD_STACK_MIN_REPLICAS=2
CONFIG_VLLM_PROD_STACK_MAX_REPLICAS=5
CONFIG_VLLM_PROD_STACK_TARGET_GPU_UTILIZATION=80

# Model configuration
CONFIG_VLLM_MODEL_URL="facebook/opt-125m"
CONFIG_VLLM_MODEL_NAME="opt-125m"

# GPU configuration - EXPLICITLY DISABLED CPU INFERENCE
# CONFIG_VLLM_USE_CPU_INFERENCE is not set
CONFIG_VLLM_REQUEST_GPU=1
CONFIG_VLLM_GPU_TYPE=""
CONFIG_VLLM_GPU_MEMORY_UTILIZATION="0.5"
CONFIG_VLLM_TENSOR_PARALLEL_SIZE=1

# Engine configuration for GPU
CONFIG_VLLM_REPLICA_COUNT=1
CONFIG_VLLM_REQUEST_CPU=8
CONFIG_VLLM_REQUEST_MEMORY="16Gi"
CONFIG_VLLM_MAX_MODEL_LEN=1024
CONFIG_VLLM_DTYPE="auto"

# Router and observability
CONFIG_VLLM_ROUTER_ENABLED=y
CONFIG_VLLM_ROUTER_ROUND_ROBIN=y
CONFIG_VLLM_OBSERVABILITY_ENABLED=y
CONFIG_VLLM_GRAFANA_PORT=3000
CONFIG_VLLM_PROMETHEUS_PORT=9090

# API configuration
CONFIG_VLLM_API_PORT=8000
CONFIG_VLLM_API_KEY=""
CONFIG_VLLM_HF_TOKEN=""

# Benchmarking
CONFIG_VLLM_BENCHMARK_ENABLED=y
CONFIG_VLLM_BENCHMARK_DURATION=60
CONFIG_VLLM_BENCHMARK_CONCURRENT_USERS=10
CONFIG_VLLM_BENCHMARK_RESULTS_DIR="/data/vllm-benchmark"
