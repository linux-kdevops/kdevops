# AWS G5.12xlarge GPU instance with 4x NVIDIA A10G
#
# Instance specifications:
# - 4x NVIDIA A10G 24GB GPUs
# - 48 vCPUs (AMD EPYC 7R32)
# - 192 GiB memory
# - 3.8 TB NVMe SSD local storage
# - 40 Gbps network performance
# - ~$5.67/hour on-demand pricing
#
# Ideal for ML training with multi-GPU support

CONFIG_TERRAFORM=y
CONFIG_TERRAFORM_AWS=y
CONFIG_TERRAFORM_AWS_DATA_ENABLE=y
CONFIG_TERRAFORM_SSH_CONFIG_UPDATE=y
CONFIG_TERRAFORM_SSH_CONFIG_STRICT_HOST_KEY_CHECKING_ASK=y
CONFIG_TERRAFORM_SSH_CONFIG=y
CONFIG_TERRAFORM_SSH_CONFIG_BACKUP=y
CONFIG_TERRAFORM_EXTRA_VARS_AUTO=y

# Use us-east-1 for best GPU availability
CONFIG_TERRAFORM_AWS_REGION_US_EAST_1=y
CONFIG_TERRAFORM_AWS_AZ_US_EAST_1A=y

# G5 instance family with 4x A10G GPUs
CONFIG_TERRAFORM_AWS_INSTANCE_TYPE_G5=y
CONFIG_TERRAFORM_AWS_INSTANCE_G5_12XLARGE=y

# Use Deep Learning PyTorch Ubuntu AMI
CONFIG_TERRAFORM_AWS_USE_GPU_AMI=y
CONFIG_TERRAFORM_AWS_GPU_AMI_DEEP_LEARNING=y

# Use default VPC
# CONFIG_TERRAFORM_AWS_CREATE_VPC is not set
CONFIG_TERRAFORM_AWS_ASSIGN_PUBLIC_IP=y

# EBS volumes for ML datasets and checkpoints
CONFIG_TERRAFORM_AWS_EBS_VOLUMES_PER_INSTANCE=4
CONFIG_TERRAFORM_AWS_EBS_VOLUME_SIZE=100
CONFIG_TERRAFORM_AWS_EBS_VOLUME_TYPE_GP3=y

# DevOps workflow
CONFIG_WORKFLOWS_DEDICATED_WORKFLOW=y
CONFIG_KDEVOPS_WORKFLOW_ENABLE_GITR=y
CONFIG_GITR=y
CONFIG_GITR_INIT=y
CONFIG_GITR_CUSTOM_COMMAND="nvidia-smi && python -c 'import torch; print(torch.cuda.device_count(), \"GPUs available\")'"