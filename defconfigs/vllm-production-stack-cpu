# vLLM Production Stack configuration with official Helm chart
CONFIG_KDEVOPS_FIRST_RUN=n
CONFIG_LIBVIRT=y
CONFIG_LIBVIRT_VCPUS=64
CONFIG_LIBVIRT_MEM_64G=y

# Workflow configuration
CONFIG_WORKFLOWS=y
CONFIG_WORKFLOWS_TESTS=y
CONFIG_WORKFLOWS_LINUX_TESTS=y
CONFIG_WORKFLOWS_DEDICATED_WORKFLOW=y
CONFIG_KDEVOPS_WORKFLOW_DEDICATE_VLLM=y

# vLLM Production Stack specific configuration
CONFIG_VLLM_PRODUCTION_STACK=y
CONFIG_VLLM_K8S_MINIKUBE=y
CONFIG_VLLM_VERSION_LATEST=y
CONFIG_VLLM_HELM_RELEASE_NAME="vllm-prod"
CONFIG_VLLM_HELM_NAMESPACE="vllm-system"
CONFIG_VLLM_PROD_STACK_REPO="https://vllm-project.github.io/production-stack"
CONFIG_VLLM_PROD_STACK_CHART_VERSION="latest"
CONFIG_VLLM_PROD_STACK_ROUTER_IMAGE="ghcr.io/vllm-project/production-stack/router"
CONFIG_VLLM_PROD_STACK_ROUTER_TAG="latest"
CONFIG_VLLM_PROD_STACK_ENABLE_MONITORING=y
CONFIG_VLLM_PROD_STACK_ENABLE_AUTOSCALING=n
CONFIG_VLLM_MODEL_URL="facebook/opt-125m"
CONFIG_VLLM_MODEL_NAME="opt-125m"
CONFIG_VLLM_REPLICA_COUNT=2
CONFIG_VLLM_USE_CPU_INFERENCE=y
CONFIG_VLLM_REQUEST_CPU=8
CONFIG_VLLM_REQUEST_MEMORY="20Gi"
CONFIG_VLLM_REQUEST_GPU=0
CONFIG_VLLM_MAX_MODEL_LEN=2048
CONFIG_VLLM_DTYPE="float32"
CONFIG_VLLM_TENSOR_PARALLEL_SIZE=1
CONFIG_VLLM_ROUTER_ENABLED=y
CONFIG_VLLM_ROUTER_ROUND_ROBIN=y
CONFIG_VLLM_OBSERVABILITY_ENABLED=y
CONFIG_VLLM_GRAFANA_PORT=3000
CONFIG_VLLM_PROMETHEUS_PORT=9090
CONFIG_VLLM_API_PORT=8000
CONFIG_VLLM_BENCHMARK_ENABLED=y
CONFIG_VLLM_BENCHMARK_DURATION=60
CONFIG_VLLM_BENCHMARK_CONCURRENT_USERS=10
CONFIG_VLLM_BENCHMARK_RESULTS_DIR="/data/vllm-benchmark"
