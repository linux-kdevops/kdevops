# AWS P5.48xlarge GPU instance with 8x NVIDIA H100
#
# Instance specifications:
# - 8x NVIDIA H100 80GB GPUs
# - 192 vCPUs (AMD EPYC)
# - 2,048 GiB memory
# - 30 TB NVMe SSD local storage
# - 3200 Gbps network with EFA v2 support
# - ~$98.32/hour on-demand pricing
#
# Top-tier instance for the most demanding AI/ML workloads

CONFIG_TERRAFORM=y
CONFIG_TERRAFORM_AWS=y
CONFIG_TERRAFORM_AWS_DATA_ENABLE=y
CONFIG_TERRAFORM_SSH_CONFIG_UPDATE=y
CONFIG_TERRAFORM_SSH_CONFIG_STRICT_HOST_KEY_CHECKING_ASK=y
CONFIG_TERRAFORM_SSH_CONFIG=y
CONFIG_TERRAFORM_SSH_CONFIG_BACKUP=y
CONFIG_TERRAFORM_EXTRA_VARS_AUTO=y

# Use us-east-1 for best GPU availability
CONFIG_TERRAFORM_AWS_REGION_US_EAST_1=y
CONFIG_TERRAFORM_AWS_AZ_US_EAST_1A=y

# P5 instance family with 8x H100 GPUs
CONFIG_TERRAFORM_AWS_INSTANCE_TYPE_P5=y
CONFIG_TERRAFORM_AWS_INSTANCE_P5_48XLARGE=y

# Use Deep Learning PyTorch Ubuntu AMI
CONFIG_TERRAFORM_AWS_USE_GPU_AMI=y
CONFIG_TERRAFORM_AWS_GPU_AMI_DEEP_LEARNING=y

# Use default VPC
# CONFIG_TERRAFORM_AWS_CREATE_VPC is not set
CONFIG_TERRAFORM_AWS_ASSIGN_PUBLIC_IP=y

# EBS volumes for ML datasets and checkpoints
CONFIG_TERRAFORM_AWS_EBS_VOLUMES_PER_INSTANCE=4
CONFIG_TERRAFORM_AWS_EBS_VOLUME_SIZE=200
CONFIG_TERRAFORM_AWS_EBS_VOLUME_TYPE_GP3=y

# DevOps workflow
CONFIG_WORKFLOWS_DEDICATED_WORKFLOW=y
CONFIG_KDEVOPS_WORKFLOW_ENABLE_GITR=y
CONFIG_GITR=y
CONFIG_GITR_INIT=y
CONFIG_GITR_CUSTOM_COMMAND="nvidia-smi && python -c 'import torch; print(f\"H100s available: {torch.cuda.device_count()}\")'"