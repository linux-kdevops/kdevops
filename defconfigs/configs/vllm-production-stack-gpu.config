# vLLM Production Stack with GPU support
CONFIG_WORKFLOWS=y
CONFIG_WORKFLOWS_TESTS=y
CONFIG_WORKFLOWS_LINUX_TESTS=y
CONFIG_WORKFLOWS_DEDICATED_WORKFLOW=y
CONFIG_KDEVOPS_WORKFLOW_DEDICATE_VLLM=y

# vLLM Production Stack with Kubernetes
CONFIG_VLLM_PRODUCTION_STACK=y
CONFIG_VLLM_K8S_MINIKUBE=y
CONFIG_VLLM_VERSION_STABLE=y
CONFIG_VLLM_ENGINE_IMAGE_TAG="v0.10.2"
CONFIG_VLLM_HELM_RELEASE_NAME="vllm-prod"
CONFIG_VLLM_HELM_NAMESPACE="vllm-system"

# Production Stack components
CONFIG_VLLM_PROD_STACK_REPO="https://vllm-project.github.io/production-stack"
CONFIG_VLLM_PROD_STACK_CHART_VERSION="latest"
CONFIG_VLLM_PROD_STACK_ROUTER_IMAGE="ghcr.io/vllm-project/production-stack/router"
CONFIG_VLLM_PROD_STACK_ROUTER_TAG="latest"
CONFIG_VLLM_PROD_STACK_ENABLE_MONITORING=y
CONFIG_VLLM_PROD_STACK_ENABLE_AUTOSCALING=y
CONFIG_VLLM_PROD_STACK_MIN_REPLICAS=2
CONFIG_VLLM_PROD_STACK_MAX_REPLICAS=5
CONFIG_VLLM_PROD_STACK_TARGET_GPU_UTILIZATION=80

# Model configuration
CONFIG_VLLM_MODEL_URL="facebook/opt-125m"
CONFIG_VLLM_MODEL_NAME="opt-125m"

# GPU configuration - EXPLICITLY DISABLED CPU INFERENCE
# CONFIG_VLLM_USE_CPU_INFERENCE is not set
CONFIG_VLLM_REQUEST_GPU=1
CONFIG_VLLM_GPU_TYPE=""
CONFIG_VLLM_GPU_MEMORY_UTILIZATION="0.5"
CONFIG_VLLM_TENSOR_PARALLEL_SIZE=1

# Engine configuration for GPU
CONFIG_VLLM_REPLICA_COUNT=1
CONFIG_VLLM_REQUEST_CPU=8
CONFIG_VLLM_REQUEST_MEMORY="16Gi"
CONFIG_VLLM_MAX_MODEL_LEN=1024
CONFIG_VLLM_DTYPE="auto"

# Router and observability
CONFIG_VLLM_ROUTER_ENABLED=y
CONFIG_VLLM_ROUTER_ROUND_ROBIN=y
CONFIG_VLLM_OBSERVABILITY_ENABLED=y
CONFIG_VLLM_GRAFANA_PORT=3000
CONFIG_VLLM_PROMETHEUS_PORT=9090

# API configuration
CONFIG_VLLM_API_PORT=8000
CONFIG_VLLM_API_KEY=""
CONFIG_VLLM_HF_TOKEN=""

# Benchmarking
CONFIG_VLLM_BENCHMARK_ENABLED=y
CONFIG_VLLM_BENCHMARK_DURATION=60
CONFIG_VLLM_BENCHMARK_CONCURRENT_USERS=10
CONFIG_VLLM_BENCHMARK_RESULTS_DIR="/data/vllm-benchmark"
